{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix, find, bmat, coo_matrix\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import calinski_harabaz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = 'train.dat'\n",
    "data = open(file, 'r')\n",
    "#print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8580\n"
     ]
    }
   ],
   "source": [
    "docs = list()\n",
    "for row in data:\n",
    "    docs.append(row.rstrip().split(\" \"))    \n",
    "print(len(docs))\n",
    "#docs = docs[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csr_idf(mat, copy=False, **kargs):\n",
    "    r\"\"\" Scale a CSR matrix by idf. \n",
    "    Returns scaling factors as dict. If copy is True, \n",
    "    returns scaled matrix and scaling factors.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # document frequency\n",
    "    df = defaultdict(int)\n",
    "    for i in ind:\n",
    "        df[i] += 1\n",
    "    # inverse document frequency\n",
    "    for k,v in df.items():\n",
    "        df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    # scale by idf\n",
    "    for i in range(0, nnz):\n",
    "        val[i] *= df[ind[i]]\n",
    "        \n",
    "    return df if copy is False else mat\n",
    "\n",
    "def csr_build(dataIndex, value, nnz, nrows):\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0\n",
    "    n = 0\n",
    "    \n",
    "    for (d,v) in zip(dataIndex, value):\n",
    "        l = len(d)\n",
    "        for j in range(l):\n",
    "#             print j, k\n",
    "            ind[int(j) + n] = d[j]\n",
    "            val[int(j) + n] = v[j]\n",
    "        \n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "    \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, max(ind)+1), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def a(docs):\n",
    "    # sperate indices and values\n",
    "    dataIndex = list()\n",
    "    value = list()\n",
    "    for d in docs:\n",
    "        d_index = list()\n",
    "        d_value = list()\n",
    "        for i in range(0,len(d),2):      \n",
    "            d_index.append(d[i])\n",
    "        for j in range(1,len(d),2):     \n",
    "            d_value.append(d[j])\n",
    "        dataIndex.append(d_index)\n",
    "        value.append(d_value)\n",
    "        \n",
    "    nrows = len(docs)\n",
    "\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    ncol = 0\n",
    "    _max = list()\n",
    "    for d in dataIndex:\n",
    "        nnz += len(d)\n",
    "        _max.append(max(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "#             print(w)\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    mat1 = csr_build(dataIndex, value, nnz, nrows)\n",
    "    mat1 = csr_idf(mat1, copy=True)\n",
    "    nrows = mat1.shape[0]\n",
    "    nnz = mat1.nnz\n",
    "    ind, val, ptr = mat1.indices, mat1.data, mat1.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = float(1.0/np.sqrt(rsum))\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "    return mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat3 = a(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "938 point1\n",
      "8238 point2\n",
      "[[ 1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "A = mat3.todense()\n",
    "maxItr = 0\n",
    "print A\n",
    "length = mat3.shape[0];\n",
    "for i in range(0,length/2):\n",
    "    for j in range(length/2+1,length):\n",
    "        distance = euclidean_distances(A[i], A[j])\n",
    "        if  distance > maxItr:\n",
    "            maxItr = distance\n",
    "            point1 = i\n",
    "            point2 = j\n",
    "\n",
    "print point1,\"point1\"\n",
    "print point2,\"point2\"\n",
    "print maxItr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sse(l1):\n",
    "    su = 0\n",
    "    A = coo_matrix(l1)\n",
    "    mean = np.mean(A.data)\n",
    "    for i,j,v in zip(A.row, A.col, A.data):\n",
    "        su = su + ((mean - v)**2)\n",
    "    return su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initCentorids(x, k):\n",
    "    x_shuffle = shuffle(x, n_samples=2)\n",
    "    return x_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim(x1, x2):\n",
    "    sims = x1.dot(x2.T)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findCentroids(matc, centroids):\n",
    "    idx = list()\n",
    "    simsMatrix = sim(matc, centroids)\n",
    "\n",
    "    for i in range(simsMatrix.shape[0]):\n",
    "        row = simsMatrix.getrow(i).toarray()[0].ravel()\n",
    "        top_indices = row.argsort()[-1]\n",
    "        idx.append(top_indices + 1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeMeans(matm, idx, k):\n",
    "    centroids = list()\n",
    "    for i in range(1,k+1):\n",
    "        indi = [j for j, x in enumerate(idx) if x == i]\n",
    "        members = matm[indi,:]\n",
    "        if (members.shape[0] > 1):\n",
    "            centroids.append(members.toarray().mean(0))\n",
    "    centroids_csr = csr_matrix(centroids)\n",
    "    return centroids_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = [[]]\n",
    "def divide(ids):\n",
    "    global m\n",
    "    c1 = list()\n",
    "    c2 = list()\n",
    "    #print m, \"====================m divide===============\"\n",
    "    if len(m[0]) == 0 :\n",
    "        for j in range(len(ids)):\n",
    "            if ids[j] == 1:\n",
    "                c1.append(j)\n",
    "            else :\n",
    "                c2.append(j)\n",
    "    else :\n",
    "        for k in range(len(ids)):\n",
    "            if ids[k] == 1:\n",
    "                c1.append(m[0][k])\n",
    "            else :\n",
    "                c2.append(m[0][k])\n",
    "    return c1, c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans(k, matk, n_iter):\n",
    "    i = 0\n",
    "    centroids = initCentorids(matk, k)\n",
    "    old_sse = []\n",
    "    new_sse = [0]*2\n",
    "    min_sse = 0.00001\n",
    "    for _ in range(n_iter):\n",
    "        print _\n",
    "        idx = findCentroids(matk, centroids)\n",
    "        clust1, clust2 = divide(idx)\n",
    "        if i == 0:\n",
    "            old_sse.append(calculate_sse(clust1))\n",
    "            old_sse.append(calculate_sse(clust2))\n",
    "            i = i+1;\n",
    "        else :\n",
    "            new_sse[0] = calculate_sse(clust1)\n",
    "            new_sse[1] = calculate_sse(clust2)\n",
    "            if (np.abs(old_sse[0]-new_sse[0]) <= min_sse) or (np.abs(old_sse[1]-new_sse[1]) <= min_sse) :\n",
    "                print _, \"=======================niter==================\"\n",
    "                return idx, clust1, clust2\n",
    "            else :\n",
    "                old_sse = new_sse[:]\n",
    "        print old_sse\n",
    "        centroids = computeMeans(matk, idx, k)        \n",
    "    return idx, clust1, clust2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_csr(l1):\n",
    "    mcsr = list()\n",
    "#    print l1\n",
    "    for i in l1:\n",
    "        mcsr.append(docs[i])\n",
    "    return a(mcsr)\n",
    "#convert_to_csr([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unselected_clusters = set()\n",
    "def choose_cluster(list_of_clusters):\n",
    "    global unselected_clusters\n",
    "    l2 = list()\n",
    "    var =[]\n",
    "    sse = []\n",
    "    for i in list_of_clusters:\n",
    "        unselected_clusters.add(tuple(i))\n",
    "    greatest = 0\n",
    "\n",
    "    for i in unselected_clusters:\n",
    "        l2.append(convert_to_csr(i))\n",
    "    for l in l2:\n",
    "        sse.append(calculate_sse(l))\n",
    "\n",
    "    #print sse, \"=======================sse=======================\"\n",
    "    csr_l2 = l2[(sse.index(max(sse)))]\n",
    "    indx = list(unselected_clusters)[(sse.index(max(sse)))]\n",
    "    unselected_clusters.remove(indx)\n",
    "    return indx, csr_l2\n",
    "#choose_cluster([[1,2,3],[7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp = list()\n",
    "def bisecting(matb):\n",
    "    temp=matb.copy()\n",
    "    global m\n",
    "    for i in range(1, 7):\n",
    "        ids, clust1, clust2 = kmeans(2, temp, 40)\n",
    "        #print clust1 , \"=========================c1========================\"\n",
    "        #print clust2, \"============================c2======================\"\n",
    "        #print \"#######################################################################\"\n",
    "        # m = chosen cluster indexes, m1 = not selected\n",
    "        im, smat = choose_cluster([clust1, clust2])\n",
    "        m = [im]\n",
    "        #print m , \"========================================m1 bisectibg============================\"\n",
    "        # marix at the indexes of the choosen cluster \"m\"\n",
    "        temp = smat.copy()\n",
    "    for i in m:\n",
    "        unselected_clusters.add(tuple(i))\n",
    "    return unselected_clusters\n",
    "        \n",
    "print \"====================================\"\n",
    "out = bisecting(mat3)\n",
    "#print out\n",
    "output_file = open(\"sse-kmeans+bisect-40-itr-0.401-30nov-1.dat\", \"w\")\n",
    "z = 0\n",
    "o = [0] * 8580\n",
    "for x in out:\n",
    "    z = z + 1\n",
    "    for y in x:\n",
    "        o[y] = z\n",
    "#print o\n",
    "for w in o:\n",
    "    output_file.write(str(w))\n",
    "    output_file.write('\\n')\n",
    "output_file.close()\n",
    "\"\"\"def printFile(outs):\n",
    "    output_file = open(\"output.dat\", \"w\")\n",
    "    \n",
    "printFile(out)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
